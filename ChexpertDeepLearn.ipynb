{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChexpertDeepLearn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjg133/AI_Healthcare_Jay/blob/master/ChexpertDeepLearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pQVDSAsD25U",
        "colab_type": "code",
        "outputId": "d22b96bc-77ad-4d91-8a6c-69018cde71f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYdB1vaTxn41",
        "colab_type": "code",
        "outputId": "9353ac1b-34f9-4baf-9f2a-49a259b365b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!unzip '/content/drive/My Drive/CheXpert-v1.0-small.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open /content/drive/My Drive/CheXpert-v1.0-small.zip, /content/drive/My Drive/CheXpert-v1.0-small.zip.zip or /content/drive/My Drive/CheXpert-v1.0-small.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOQgKVm4D5gT",
        "colab_type": "code",
        "outputId": "2df51086-cd2b-4493-d407-f42fc7cf1176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.applications.resnet import ResNet152\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#filepath_c=\"/content/drive/My Drive/MyCNN/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "filepath_c=\"/content/drive/My Drive/MyCNN/model.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath_c, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "filepath = \"/content/drive/My Drive/\"\n",
        "training = pd.read_csv(filepath + \"CheXpert-v1.0-small/train.csv\")\n",
        "# use 20,000 samples, row = 83052\n",
        "i = 0\n",
        "train_image = []\n",
        "for row in training.itertuples():\n",
        "  img = image.load_img(filepath+row.Path, target_size=(320,320), color_mode='rgb')\n",
        "  img = image.img_to_array(img)\n",
        "  train_image.append(img)\n",
        "  i = i + 1\n",
        "  if i % 20 == 0:\n",
        "    print(i)\n",
        "  if i > 1000:\n",
        "    break\n",
        "X = np.array(train_image)\n",
        "Y = training.iloc[0:1001, 5:]\n",
        "\n",
        "testing = pd.read_csv(filepath + \"CheXpert-v1.0-small/valid.csv\")\n",
        "test_image = []\n",
        "for row in testing.itertuples():\n",
        "  img = image.load_img(filepath+row.Path, target_size=(320,320), color_mode='rgb')\n",
        "  img = image.img_to_array(img)\n",
        "  test_image.append(img)\n",
        "xtest = np.array(test_image)\n",
        "ytest = testing.iloc[:,5:]\n",
        "\n",
        "print(\"Done loading data\")\n",
        "\n",
        "xtrain = X\n",
        "ytrain = Y\n",
        "\n",
        "model = ResNet50(weights='imagenet', input_shape=(320,320,3), include_top=False, pooling='avg')\n",
        "model.summary()\n",
        "#flat1 = Flatten()(model.output)\n",
        "#class1 = Dense(2048, activation='relu')(flat1)\n",
        "output = Dense(14, activation='softmax')(model.output)\n",
        "model = Model(inputs=model.input, outputs=output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(\"Now fitting model\")\n",
        "# Adjusting epochs and batch size to make training faster\n",
        "datagen = ImageDataGenerator()\n",
        "history = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=64),\n",
        " epochs=10,\n",
        " verbose=1,\n",
        " validation_data=(xtest, ytest),\n",
        " callbacks=callbacks_list)\n",
        "\n",
        "print(\"model done fitting\")\n",
        "#history = model.fit(xtrain, ytrain, validation_data=(xtest, ytest), epochs=10, batch_size=32, verbose=1) # use 5 or 10 for epochs\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "120\n",
            "140\n",
            "160\n",
            "180\n",
            "200\n",
            "220\n",
            "240\n",
            "260\n",
            "280\n",
            "300\n",
            "320\n",
            "340\n",
            "360\n",
            "380\n",
            "400\n",
            "420\n",
            "440\n",
            "460\n",
            "480\n",
            "500\n",
            "520\n",
            "540\n",
            "560\n",
            "580\n",
            "600\n",
            "620\n",
            "640\n",
            "660\n",
            "680\n",
            "700\n",
            "720\n",
            "740\n",
            "760\n",
            "780\n",
            "800\n",
            "820\n",
            "840\n",
            "860\n",
            "880\n",
            "900\n",
            "920\n",
            "940\n",
            "960\n",
            "980\n",
            "1000\n",
            "Done loading data\n",
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 320, 320, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 326, 326, 3)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 160, 160, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 160, 160, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 160, 160, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 162, 162, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 80, 80, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 80, 80, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 80, 80, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 80, 80, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 80, 80, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 80, 80, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 80, 80, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 80, 80, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 80, 80, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 80, 80, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 80, 80, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 80, 80, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 80, 80, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 80, 80, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 80, 80, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 80, 80, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 80, 80, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 80, 80, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 80, 80, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 80, 80, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 80, 80, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 80, 80, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 80, 80, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 80, 80, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 80, 80, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 80, 80, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 80, 80, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 80, 80, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 80, 80, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 80, 80, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 80, 80, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 80, 80, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 80, 80, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 40, 40, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 40, 40, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 40, 40, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 40, 40, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 40, 40, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 40, 40, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 40, 40, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 40, 40, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 40, 40, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 40, 40, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 40, 40, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 40, 40, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 40, 40, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 40, 40, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 40, 40, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 40, 40, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 40, 40, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 40, 40, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 40, 40, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 40, 40, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 40, 40, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 40, 40, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 40, 40, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 40, 40, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 40, 40, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 40, 40, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 40, 40, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 40, 40, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 40, 40, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 40, 40, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 40, 40, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 40, 40, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 40, 40, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 40, 40, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 40, 40, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 40, 40, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 40, 40, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 40, 40, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 40, 40, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 40, 40, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 40, 40, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 40, 40, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 20, 20, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 20, 20, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 20, 20, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 20, 20, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 20, 20, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 20, 20, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 20, 20, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 20, 20, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 20, 20, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 20, 20, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 20, 20, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 20, 20, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 20, 20, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 20, 20, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 20, 20, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 20, 20, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 20, 20, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 20, 20, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 20, 20, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 20, 20, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 20, 20, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 20, 20, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 20, 20, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 20, 20, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 20, 20, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 20, 20, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 20, 20, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 20, 20, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 20, 20, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 20, 20, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 20, 20, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 20, 20, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 20, 20, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 20, 20, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 20, 20, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 20, 20, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 20, 20, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 20, 20, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 20, 20, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 20, 20, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 20, 20, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 20, 20, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 20, 20, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 20, 20, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 20, 20, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 20, 20, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 20, 20, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 20, 20, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 20, 20, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 20, 20, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 20, 20, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 20, 20, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 20, 20, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 20, 20, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 20, 20, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 20, 20, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 20, 20, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 20, 20, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 20, 20, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 20, 20, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 20, 20, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 20, 20, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 10, 10, 512)  524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 10, 10, 512)  0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 10, 10, 512)  2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 10, 10, 512)  0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 10, 10, 2048) 2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 10, 10, 2048) 8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 10, 10, 2048) 8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 10, 10, 2048) 0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 10, 10, 2048) 0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 10, 10, 512)  1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 10, 10, 512)  0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 10, 10, 512)  2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 10, 10, 512)  0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 10, 10, 2048) 8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 10, 10, 2048) 0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 10, 10, 2048) 0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 10, 10, 512)  1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 10, 10, 512)  0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 10, 10, 512)  2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 10, 10, 512)  0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 10, 10, 2048) 8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 10, 10, 2048) 0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 10, 10, 2048) 0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "Now fitting model\n",
            "Epoch 1/10\n",
            "16/16 [==============================] - 212s 13s/step - loss: nan - acc: 0.1332 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.20085, saving model to /content/drive/My Drive/MyCNN/model.h5\n",
            "Epoch 2/10\n",
            " 6/16 [==========>...................] - ETA: 1:54 - loss: nan - acc: 0.1387"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UytZzSn6W2nu",
        "colab_type": "text"
      },
      "source": [
        "This section of code is to resume the training from the above code, if the above code times out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atyX7GnYNAvg",
        "colab_type": "code",
        "outputId": "aea04e07-ff81-4d7a-877a-4d26c2a7e88d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.applications.resnet import ResNet152\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#filepath_c=\"/content/drive/My Drive/MyCNN/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "filepath_c=\"/content/drive/My Drive/MyCNN/model.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath_c, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "filepath = \"/content/drive/My Drive/\"\n",
        "training = pd.read_csv(filepath + \"CheXpert-v1.0-small/train.csv\")\n",
        "# use 20,000 samples, row = 83052\n",
        "i = 0\n",
        "train_image = []\n",
        "for row in training.itertuples():\n",
        "  img = image.load_img(filepath+row.Path, target_size=(320,320), color_mode='rgb')\n",
        "  img = image.img_to_array(img)\n",
        "  train_image.append(img)\n",
        "  i = i + 1\n",
        "  if i % 20 == 0:\n",
        "    print(i)\n",
        "  if i > 5000:\n",
        "    break\n",
        "X = np.array(train_image)\n",
        "Y = training.iloc[0:5001, 5:]\n",
        "\n",
        "testing = pd.read_csv(filepath + \"CheXpert-v1.0-small/valid.csv\")\n",
        "test_image = []\n",
        "for row in testing.itertuples():\n",
        "  img = image.load_img(filepath+row.Path, target_size=(320,320), color_mode='rgb')\n",
        "  img = image.img_to_array(img)\n",
        "  test_image.append(img)\n",
        "xtest = np.array(test_image)\n",
        "ytest = testing.iloc[:,5:]\n",
        "\n",
        "print(\"Done loading data\")\n",
        "\n",
        "xtrain = X\n",
        "ytrain = Y\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "filepath_c=\"/content/drive/My Drive/MyCNN/model.h5\"\n",
        "model = load_model(filepath_c)\n",
        "datagen = ImageDataGenerator()\n",
        "# change 047 and 0.905 to get correct weights file\n",
        "#model.load_weights('/content/drive/My Drive/MyCNN/epochs:047-val_acc:0.905.hdf5')\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(\"Resuming Training\")\n",
        "history = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=64),\n",
        " epochs=50,\n",
        " verbose=1,\n",
        " validation_data=(xtest, ytest),\n",
        " callbacks=callbacks_list)\n",
        "print(\"model done fitting\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "120\n",
            "140\n",
            "160\n",
            "180\n",
            "200\n",
            "220\n",
            "240\n",
            "260\n",
            "280\n",
            "300\n",
            "320\n",
            "340\n",
            "360\n",
            "380\n",
            "400\n",
            "420\n",
            "440\n",
            "460\n",
            "480\n",
            "500\n",
            "520\n",
            "540\n",
            "560\n",
            "580\n",
            "600\n",
            "620\n",
            "640\n",
            "660\n",
            "680\n",
            "700\n",
            "720\n",
            "740\n",
            "760\n",
            "780\n",
            "800\n",
            "820\n",
            "840\n",
            "860\n",
            "880\n",
            "900\n",
            "920\n",
            "940\n",
            "960\n",
            "980\n",
            "1000\n",
            "1020\n",
            "1040\n",
            "1060\n",
            "1080\n",
            "1100\n",
            "1120\n",
            "1140\n",
            "1160\n",
            "1180\n",
            "1200\n",
            "1220\n",
            "1240\n",
            "1260\n",
            "1280\n",
            "1300\n",
            "1320\n",
            "1340\n",
            "1360\n",
            "1380\n",
            "1400\n",
            "1420\n",
            "1440\n",
            "1460\n",
            "1480\n",
            "1500\n",
            "1520\n",
            "1540\n",
            "1560\n",
            "1580\n",
            "1600\n",
            "1620\n",
            "1640\n",
            "1660\n",
            "1680\n",
            "1700\n",
            "1720\n",
            "1740\n",
            "1760\n",
            "1780\n",
            "1800\n",
            "1820\n",
            "1840\n",
            "1860\n",
            "1880\n",
            "1900\n",
            "1920\n",
            "1940\n",
            "1960\n",
            "1980\n",
            "2000\n",
            "2020\n",
            "2040\n",
            "2060\n",
            "2080\n",
            "2100\n",
            "2120\n",
            "2140\n",
            "2160\n",
            "2180\n",
            "2200\n",
            "2220\n",
            "2240\n",
            "2260\n",
            "2280\n",
            "2300\n",
            "2320\n",
            "2340\n",
            "2360\n",
            "2380\n",
            "2400\n",
            "2420\n",
            "2440\n",
            "2460\n",
            "2480\n",
            "2500\n",
            "2520\n",
            "2540\n",
            "2560\n",
            "2580\n",
            "2600\n",
            "2620\n",
            "2640\n",
            "2660\n",
            "2680\n",
            "2700\n",
            "2720\n",
            "2740\n",
            "2760\n",
            "2780\n",
            "2800\n",
            "2820\n",
            "2840\n",
            "2860\n",
            "2880\n",
            "2900\n",
            "2920\n",
            "2940\n",
            "2960\n",
            "2980\n",
            "3000\n",
            "3020\n",
            "3040\n",
            "3060\n",
            "3080\n",
            "3100\n",
            "3120\n",
            "3140\n",
            "3160\n",
            "3180\n",
            "3200\n",
            "3220\n",
            "3240\n",
            "3260\n",
            "3280\n",
            "3300\n",
            "3320\n",
            "3340\n",
            "3360\n",
            "3380\n",
            "3400\n",
            "3420\n",
            "3440\n",
            "3460\n",
            "3480\n",
            "3500\n",
            "3520\n",
            "3540\n",
            "3560\n",
            "3580\n",
            "3600\n",
            "3620\n",
            "3640\n",
            "3660\n",
            "3680\n",
            "3700\n",
            "3720\n",
            "3740\n",
            "3760\n",
            "3780\n",
            "3800\n",
            "3820\n",
            "3840\n",
            "3860\n",
            "3880\n",
            "3900\n",
            "3920\n",
            "3940\n",
            "3960\n",
            "3980\n",
            "4000\n",
            "4020\n",
            "4040\n",
            "4060\n",
            "4080\n",
            "4100\n",
            "4120\n",
            "4140\n",
            "4160\n",
            "4180\n",
            "4200\n",
            "4220\n",
            "4240\n",
            "4260\n",
            "4280\n",
            "4300\n",
            "4320\n",
            "4340\n",
            "4360\n",
            "4380\n",
            "4400\n",
            "4420\n",
            "4440\n",
            "4460\n",
            "4480\n",
            "4500\n",
            "4520\n",
            "4540\n",
            "4560\n",
            "4580\n",
            "4600\n",
            "4620\n",
            "4640\n",
            "4660\n",
            "4680\n",
            "4700\n",
            "4720\n",
            "4740\n",
            "4760\n",
            "4780\n",
            "4800\n",
            "4820\n",
            "4840\n",
            "4860\n",
            "4880\n",
            "4900\n",
            "4920\n",
            "4940\n",
            "4960\n",
            "4980\n",
            "5000\n",
            "Done loading data\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Resuming Training\n",
            "Epoch 1/50\n",
            "79/79 [==============================] - 1095s 14s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.20085, saving model to /content/drive/My Drive/MyCNN/model.h5\n",
            "Epoch 2/50\n",
            "79/79 [==============================] - 1057s 13s/step - loss: nan - acc: 0.1157 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.20085\n",
            "Epoch 3/50\n",
            "79/79 [==============================] - 1073s 14s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.20085\n",
            "Epoch 4/50\n",
            "79/79 [==============================] - 1059s 13s/step - loss: nan - acc: 0.1157 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.20085\n",
            "Epoch 5/50\n",
            "79/79 [==============================] - 1042s 13s/step - loss: nan - acc: 0.1157 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.20085\n",
            "Epoch 6/50\n",
            "79/79 [==============================] - 1044s 13s/step - loss: nan - acc: 0.1181 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.20085\n",
            "Epoch 7/50\n",
            "79/79 [==============================] - 1043s 13s/step - loss: nan - acc: 0.1181 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.20085\n",
            "Epoch 8/50\n",
            "79/79 [==============================] - 1044s 13s/step - loss: nan - acc: 0.1181 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.20085\n",
            "Epoch 9/50\n",
            "79/79 [==============================] - 1047s 13s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.20085\n",
            "Epoch 10/50\n",
            "79/79 [==============================] - 1053s 13s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.20085\n",
            "Epoch 11/50\n",
            "79/79 [==============================] - 1053s 13s/step - loss: nan - acc: 0.1181 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.20085\n",
            "Epoch 12/50\n",
            "79/79 [==============================] - 1050s 13s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.20085\n",
            "Epoch 13/50\n",
            "79/79 [==============================] - 1037s 13s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.20085\n",
            "Epoch 14/50\n",
            "79/79 [==============================] - 1049s 13s/step - loss: nan - acc: 0.1181 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.20085\n",
            "Epoch 15/50\n",
            "79/79 [==============================] - 1053s 13s/step - loss: nan - acc: 0.1193 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.20085\n",
            "Epoch 16/50\n",
            "79/79 [==============================] - 1063s 13s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.20085\n",
            "Epoch 17/50\n",
            "79/79 [==============================] - 1093s 14s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.20085\n",
            "Epoch 18/50\n",
            "79/79 [==============================] - 1067s 14s/step - loss: nan - acc: 0.1181 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.20085\n",
            "Epoch 19/50\n",
            "79/79 [==============================] - 1059s 13s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.20085\n",
            "Epoch 20/50\n",
            "79/79 [==============================] - 1049s 13s/step - loss: nan - acc: 0.1181 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.20085\n",
            "Epoch 21/50\n",
            "79/79 [==============================] - 1055s 13s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.20085 to 0.20085, saving model to /content/drive/My Drive/MyCNN/model.h5\n",
            "Epoch 22/50\n",
            "79/79 [==============================] - 1064s 13s/step - loss: nan - acc: 0.1157 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.20085\n",
            "Epoch 23/50\n",
            "79/79 [==============================] - 1056s 13s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.20085\n",
            "Epoch 24/50\n",
            "79/79 [==============================] - 1076s 14s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.20085\n",
            "Epoch 25/50\n",
            "79/79 [==============================] - 1070s 14s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.20085\n",
            "Epoch 26/50\n",
            "79/79 [==============================] - 1064s 13s/step - loss: nan - acc: 0.1157 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.20085\n",
            "Epoch 27/50\n",
            "79/79 [==============================] - 1077s 14s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.20085\n",
            "Epoch 28/50\n",
            "79/79 [==============================] - 1073s 14s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.20085\n",
            "Epoch 29/50\n",
            "79/79 [==============================] - 1081s 14s/step - loss: nan - acc: 0.1193 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.20085\n",
            "Epoch 30/50\n",
            "79/79 [==============================] - 1077s 14s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.20085\n",
            "Epoch 31/50\n",
            "79/79 [==============================] - 1070s 14s/step - loss: nan - acc: 0.1157 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.20085\n",
            "Epoch 32/50\n",
            "79/79 [==============================] - 1076s 14s/step - loss: nan - acc: 0.1181 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.20085\n",
            "Epoch 33/50\n",
            "79/79 [==============================] - 1064s 13s/step - loss: nan - acc: 0.1181 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.20085\n",
            "Epoch 34/50\n",
            "79/79 [==============================] - 1069s 14s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.20085\n",
            "Epoch 35/50\n",
            "79/79 [==============================] - 1070s 14s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.20085\n",
            "Epoch 36/50\n",
            "79/79 [==============================] - 1065s 13s/step - loss: nan - acc: 0.1169 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.20085\n",
            "Epoch 37/50\n",
            "79/79 [==============================] - 1062s 13s/step - loss: nan - acc: 0.1181 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.20085\n",
            "Epoch 38/50\n",
            "79/79 [==============================] - 1055s 13s/step - loss: nan - acc: 0.1157 - val_loss: nan - val_acc: 0.2009\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.20085\n",
            "Epoch 39/50\n",
            "20/79 [======>.......................] - ETA: 13:02 - loss: nan - acc: 0.1148"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1tT4FTfXMbs",
        "colab_type": "text"
      },
      "source": [
        "This section of code is to test the model after the above training is done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5SM7mOzQnwb",
        "colab_type": "code",
        "outputId": "e8c95bac-4edf-45fc-aa29-435168eed386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from sklearn.metrics import *\n",
        "\n",
        "filepath = \"/content/drive/My Drive/\"\n",
        "testing = pd.read_csv(filepath + \"CheXpert-v1.0-small/valid.csv\")\n",
        "test_image = []\n",
        "for row in testing.itertuples():\n",
        "  img = image.load_img(filepath+row.Path, target_size=(320,320), color_mode='rgb')\n",
        "  img = image.img_to_array(img)\n",
        "  test_image.append(img)\n",
        "xtest = np.array(test_image)\n",
        "ytest = testing.iloc[:,5:]\n",
        "\n",
        "\n",
        "filepath_c=\"/content/drive/My Drive/MyCNN/model.h5\"\n",
        "model = load_model(filepath_c)\n",
        "\n",
        "# Link to evaluation metrics for deep learning: https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/\n",
        "# Basically the same as we did for the other machine learning algorithms, just make some predictions and then evaluate the predictions\n",
        "print(model.metrics_names)\n",
        "test_acc = model.evaluate(xtest, ytest, verbose=0)\n",
        "print(test_acc)\n",
        "yhat_probs = model.predict(xtest, verbose=0)\n",
        "yhat_classes = np.argmax(yhat_probs,axis=1) #model.predict_classes(xtest, verbose=0)\n",
        "print(yhat_probs)\n",
        "print(yhat_classes)\n",
        "#yhat_probs = yhat_probs[:, 0]\n",
        "#yhat_classes = yhat_classes[:, 0]\n",
        "\n",
        "#accuracy = accuracy_score(ytest, yhat_classes)\n",
        "#print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "#precision = precision_score(ytest, yhat_classes)\n",
        "#print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "#recall = recall_score(ytest, yhat_classes)\n",
        "#print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "#f1 = f1_score(ytest, yhat_classes)\n",
        "#print('F1 score: %f' % f1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['loss', 'acc']\n",
            "[nan, 0.20085470085470086]\n",
            "[[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}